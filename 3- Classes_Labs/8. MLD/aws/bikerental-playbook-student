create notebook instance
New execution role
select All S3
open jupyter

new folder
rename as bikerental
click and enter the folder
upload files 
XGBoost Cloud Prediction  Template
XGBoost Cloud Traininmg Template
bike_train.csv
bike_test.csv
bike_validation.csv

click xgboost_cloud_training_template_student.ipynb
select conda_python3 kernel

import libraries
change bucket name to your own bucket name
write-to_s3 uploads the files to s3
check s3 
Session provides credentials and region
training
get execution role 
you may see under IAM roles
sagemaker.image_uris.retrieve function bring us the xgboost


build model:
same as how you build in ml lessons
selecting a model, defining hyperparameters
additionally set instance type, count etc
specify training location as data channels

train the model:
check training jobs 
see RMSE for train, validation
after complete
check training jobs as completed
see output file in S3
model is ready to deploy

deploy model:inference
see model, endpoint configuration(created automatically)
see enpoint creating and in service by refreshing

run predictions
serializers for data to be the same as model,


LAMBDA

go to IAM service
click policies 
create Policy

go to json tab
paste 

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "sagemaker:DescribeTrainingJob",
                "sagemaker:DescribeLabelingJob",
                "sagemaker:DescribeDataQualityJobDefinition",
                "sagemaker:DescribeModelPackage",
                "sagemaker:Search",
                "sagemaker:DescribeModelPackageGroup",
                "sagemaker:DescribeApp",
                "sagemaker:GetRecord",
                "sagemaker:DescribeFlowDefinition",
                "sagemaker:DescribeAlgorithm",
                "sagemaker:GetLineageGroupPolicy",
                "sagemaker:DescribeTransformJob",
                "sagemaker:DescribeInferenceRecommendationsJob",
                "sagemaker:DescribeHumanLoop",
                "sagemaker:BatchDescribeModelPackage",
                "sagemaker:DescribeAction",
                "sagemaker:DescribeDeviceFleet",
                "sagemaker:DescribeSubscribedWorkteam",
                "sagemaker:DescribeHyperParameterTuningJob",
                "sagemaker:DescribeAutoMLJob",
                "sagemaker:DescribeWorkforce",
                "sagemaker:DescribeProcessingJob",
                "sagemaker:GetDeviceFleetReport",
                "sagemaker:DescribeEndpointConfig",
                "sagemaker:DescribeStudioLifecycleConfig",
                "sagemaker:RenderUiTemplate",
                "sagemaker:DescribeImageVersion",
                "sagemaker:BatchGetRecord",
                "sagemaker:DescribeHumanTaskUi",
                "sagemaker:GetDeviceRegistration",
                "sagemaker:DescribeProject",
                "sagemaker:GetSagemakerServicecatalogPortfolioStatus",
                "sagemaker:DescribeNotebookInstance",
                "sagemaker:DescribeAppImageConfig",
                "sagemaker:DescribeLineageGroup",
                "sagemaker:DescribeNotebookInstanceLifecycleConfig",
                "sagemaker:DescribeTrial",
                "sagemaker:DescribeContext",
                "sagemaker:DescribeModelExplainabilityJobDefinition",
                "sagemaker:DescribeEndpoint",
                "sagemaker:DescribeUserProfile",
                "sagemaker:InvokeEndpoint",
                "sagemaker:DescribeMonitoringSchedule",
                "sagemaker:DescribeEdgePackagingJob",
                "sagemaker:DescribeFeatureGroup",
                "sagemaker:DescribeModelQualityJobDefinition",
                "sagemaker:GetModelPackageGroupPolicy",
                "sagemaker:DescribeModel",
                "sagemaker:DescribePipeline",
                "sagemaker:DescribeArtifact",
                "sagemaker:DescribePipelineExecution",
                "sagemaker:DescribeWorkteam",
                "sagemaker:DescribeModelBiasJobDefinition",
                "sagemaker:DescribeCompilationJob",
                "sagemaker:BatchGetMetrics",
                "sagemaker:GetSearchSuggestions",
                "sagemaker:DescribeExperiment",
                "sagemaker:DescribeImage",
                "sagemaker:DescribeDomain",
                "sagemaker:DescribeCodeRepository",
                "sagemaker:InvokeEndpointAsync",
                "sagemaker:DescribePipelineDefinitionForExecution",
                "sagemaker:DescribeTrialComponent",
                "sagemaker:DescribeDevice"
            ],
            "Resource": "*"
        }
    ]
}


go to name
write SageMakerInvokeEndPoint
create policy



click ROLES

click CREATE ROLE

AWS SERVICE is selected as default
under AWS SERVICE "use case" select LAMBDA
click next
See ADD PERMISSIONS
search SageMakerInvokeEndPoint
select, click checkbox
clear search by clicking X
search for AWSLambdaBasicExecutionRole
select the one with orange box, click checkbox
click NEXT
write ROLE NAME "lambda-your-name"
see permissions we assigned

click  CREATE ROLE
see role created
-----
write lambda in the search bar
under SERVICES search LAMBDA
select the star next to it
left CLICK open link in new tab

CLICK CREATE FUNCTION on upper-right

WRITE function name  "lambda-your-name"
Select Python for RUNTIME
under PERMISSIONS 
CLICK "Change default execution role"
SELECT Use an existing role
click the space opened
SELECT lambda-your-name
CLICK create function on bottom-right

Copy notebook below

# Reference:
# https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/
# https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html#api-gateway-simple-proxy-for-lambda-output-format

import boto3
import math
import dateutil
import json
import os

# grab environment variables
ENDPOINT_NAME = os.environ['ENDPOINT_NAME']
client = boto3.client(service_name='sagemaker-runtime')

# Raw Data Structure: 
# datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count
# Model expects data in this format (it was trained with these features):
# season,holiday,workingday,weather,temp,atemp,humidity,windspeed,year,month,day,dayofweek,hour
def transform_data(data):
    try:
        features = data.copy()
        # Extract year, month, day, dayofweek, hour
        dt = dateutil.parser.parse(features[0])
    
        features.append(dt.year)
        features.append(dt.month)
        features.append(dt.day)
        features.append(dt.weekday())
        features.append(dt.hour)
        
        # Return the transformed data. skip datetime field
        return ','.join([str(feature) for feature in features[1:]])
        
    except Exception as err:
        print('Error when transforming: {0},{1}'.format(data,err))
        raise Exception('Error when transforming: {0},{1}'.format(data,err))
        
    
def lambda_handler(event, context):
    try:    
        print("Received event: " + json.dumps(event, indent=2))
        
        request = json.loads(json.dumps(event))
        
    
        transformed_data = [transform_data(instance['features']) for instance in request["instances"]]
        
    
        # XGBoost accepts data in CSV. It does not support JSON.
        # So, we need to submit the request in CSV format
        # Prediction for multiple observations in the same call
        result = client.invoke_endpoint(EndpointName=ENDPOINT_NAME, 
                               Body=('\n'.join(transformed_data).encode('utf-8')),
                               ContentType='text/csv')
                               
    
        result = result['Body'].read().decode('utf-8')
        
        # Apply inverse transformation to get the rental count
        
       
        print(result)
        
        
        import re # python regex module
 
        s = result
 
        pattern = r'[^0-9.]+'
 
        result = re.split(pattern,s)
        print(result)
        
        #result = result.split(',')
        
        predictions = [math.expm1(float(r)) for r in result if r != '']
        
        return {
            'statusCode': 200,
            'isBase64Encoded':False,
            'body': json.dumps(predictions)
        }

    except Exception as err:
        return {
            'statusCode': 400,
            'isBase64Encoded':False,
            'body': 'Call Failed {0}'.format(err)
        }


scroll down
see code tab, code source, lambda function tab
go and paste the file 

see configuration tab next to code tab
go and click it
see environment variables on the left pane
go and click it

click edit
new window comes
click "edit environment variable"
go and copy your endpoint name from SM console endpoints
paste it in the value box
write "ENDPOINT_NAME" FOR THE KEY BOX
click SAVE
see the variable created

go to code tab back
click deploy button next to test
see the success message

near test button click the triangle
select configure test event
scroll down
see EVENT JSON
Paste the below file

{
     "instances": [
         {
             "features": ["2012-12-19 17:00:00",4,0,1,1,16.4,20.455,50,26.0027]
             
         }
    ]
}

scroll up to event name
write "bikesharingsingle"
scroll down and save

go and click the TEST button
see the execution results tab next to lambda function tab
see the result in response
see the log in function logs

for multiple observations
near test button click the triangle
select configure test event
Create new event
scroll down
see EVENT JSON
Paste the below file

{
     "instances": [
         {
             "features": ["2012-12-19 17:00:00",4,0,1,1,16.4,20.455,50,26.0027]
             
         },
         {
             "features": ["2012-12-19 18:00:00",4,0,1,1,15.58,19.695,50,23.9994]
         },
         {
             "features": ["2012-12-10 01:00:00",4,0,1,2,14.76,18.94,100,0]
         }
    ]
}


scroll up to event name
write "bikemultipleobservation"
scroll down and save
go and click the TEST button
see the execution results tab next to lambda function tab
see the result in response
see the log in function logs


API Gateway 
search api in the console
rigt click and open in new tab
choose api type REST API
click build
pop up window comes
click ok
    IF you have created api before:
    (Go and  click create API
    go to the REST API  (and not HTTP API, not private one)
    click BUILD)

rest is default selected
click New API

write name "api-your -name"
go and click CREATE API

under actions button select "create method"
select post 
click checkmark
write lambda functiion "lambda"
select "lambda-your-name"
go and click save
pop up window comes
click ok
go and click on test
scroll down
paste the file below 

{
     "instances": [
         {
             "features": ["2012-12-19 17:00:00",4,0,1,1,16.4,20.455,50,26.0027]
             
         }
    ]
}


click test
see the result status code 200

from actions go and select "deploy api"
deployment stage select new stage
write stage name "beta"
go and click deploy
copy invoke url

go to rester in google chrome
select post
paste url
select body tab
paste the below code
select json by clicking three dots on right up corner
paste the json below

{
     "instances": [
         {
             "features": ["2012-12-19 17:00:00",4,0,1,1,16.4,20.455,50,26.0027]
             
         }
    ]
}


GO AND SEND THE REQUEST
SEE the result


Postman
Create workspace
Name it as testapi
Select post
Paste web address
Select raw
Select Jason
Paste sample one


go and create env for streamlit
create aws.py
paste file below in aws.py

from textwrap import fill
import streamlit as st
import pickle
import pandas as pd
import requests
import json
import datetime

# Raw Data
#datetime,season,holiday,workingday,weather,temp,atemp,humidity,windspeed,casual,registered,count


st.sidebar.title('Bike-rental Prediction')

html_temp = """
<div style="background-color:blue;padding:10px">
<h2 style="color:white;text-align:center;">Streamlit ML Cloud App</h2>
</div>"""
st.markdown(html_temp,unsafe_allow_html=True)


sample_one = ['2012-12-19 17:00:00',4,0,1,1,16.4,20.455,50,26.0027]



season=     st.sidebar.selectbox('What is the season?', (1,2,3,4))
holiday=     st.sidebar.selectbox('Is it holiday?', (0,1))
workingday=     st.sidebar.selectbox('Is it workingday?', (0,1))
weather=        st.sidebar.selectbox('How is the weather?', (1,2,3,4))
temp=         st.sidebar.slider('What is the temp?',1, 41, step=1)
atemp=         st.sidebar.slider('What is the atemp?',1, 45, step=1)
humidity=         st.sidebar.slider('What is the humidity?',1, 100, step=5)
windspeed=         st.sidebar.slider('What is the windspeed?',1, 56, step=3)
#casual=         st.sidebar.slider('What is the casual?',0, 367, step=20)
#registered=         st.sidebar.slider('What is the registered?',1, 886, step=50)


date1=str(st.date_input("enter date",value=datetime.date(2012, 12, 19), min_value=datetime.date(2011, 1, 1),max_value=datetime.date(2012, 12, 19)))
date2=str(st.time_input("Time", datetime.datetime.now()))
#date1=str(st.date_input("Date",datetime.datetime.now()))

url = 'https://ut6utzh58a.execute-api.us-east-1.amazonaws.com/test'

 
    



sample_one = [
date1+" "+date2,
 season,
 holiday,
 workingday,
 weather,
 temp,
 atemp,
 humidity,
 windspeed]

# Single Observation
request = {
    "instances": [
        {
            "features": sample_one
        }
    ]
}

request

response = requests.post(url, data=json.dumps(request))
result = response.json()

if result['statusCode'] == 200:
    predictions = json.loads(result['body'])
    st.write('Predicted Count: ', predictions)
else:
    print('Error',result['statusCode'], result['body'])






change url variable to your own url from API Gateway
streamlit run aws.py
send request and see results

delete endpoint
stop Notebook instance





